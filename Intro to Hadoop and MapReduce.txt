Intro to Hadoop and MapReduce
=============================

Lesson 1
========

Big Data
========

Huge data requires storage and processing solutions.

What is Big Data?
Data too big to be stored and processed on a single machine.

Challenges with Big Data:
a. Data is created fast.
b. Data from different sources in various formats.

3Vs in Big Data:
a. Volume
b. Variety
c. Velocity

Data can be stored in raw format in Hadoop unlike traditional storage solutions.

Apache Hadoop: Big data storage and processing solution.

Hadoop:
Store in HDFS
Process with MapReduce

Big data is broken down into blocks of data and distributed in a cluster.
Processing is done in distributed mode.

Hadoop Ecosystem:
1. HDFS
2. MapReduce
3. Hive: Turn SQL into MapReduce which runs on cluter to fetch data from HDFS.
4. Pig: Use scripting language, rather than MapReduce
5. Impala: Directly use SQL to query data present in HDFS.
6. Sqoop: Import/Export data from HDFS and relational databases.
7. Flume: Ingest data as it generated by external systems and put it in cluster.
8. HBase: Real time database build on top of HDFS.
9. Hue: Graphical frontend to the cluster.
10. Oozie: Workflow management tool.
11. Mahout: Machine Learning library

CDH is open source solution to manage Hadoop Ecosystem.

Lesson 2
========

HDFS and MapReduce
==================

Files are stored in Hadoop Distributed File System (HDFS)
Data stored in HDFS is broken down into block (64MB default) and stored in 
	different nodes/daemons named as datanodes. Meta data of the data is stored 
	in another node/daemon names as Namenode.

Data is replicated into multiple nodes to overcome network failure or disk
	failure.

In order to overcome namenode failure, secondary namenode is put into place.
	Also known as standby namenode.

Hadoop commands:
hadoop fs [command] [options]

For e.g
hadoop fs -ls

MapReduce:
Instead of sequential processing, MapReduce breaks the data into smaller chunks
	and process it in parallel.

Data -> Mappers (Keys, Values) -> (Shuffle and Sort) -> Reducers (Key, Values)

Daemons of MapReduce:
Job Tracker and Task Tracker (v1)
Resource Manager and Node Manager (v2)

HDFS blocks are used by mappers using MapReduce processing.

Java is preferred to write Hadoop MapReduce code, but with use of Hadoop
	Streaming, it can be written in any language, such as Python.

Running MapReduce job:
hadoop jar [streaming jar] -mapper [mapper file] -reducer [reducer file] -file
	[mapper file] -file [reduce file] -input [input dir] -output [output dir]

Hadoop restricts overwriting data at the same location in cluster.

In Hadoop, think to solve every problem in MapReduce format.

Lesson 3
========

MapReduce Code
==============

While writing mapper code, think of creating intermediate code, consisting of
	keys and values.

Shuffle and Sort is done in between mapper and reducer phase.

Reduce code process on shuffle and sorted keys and values, and generate key and
	value.

In order to test your mapper and reduce code while developing, use
cat testFile | mapper.py | sort | reducer.py
